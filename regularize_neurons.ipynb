{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Yi1xcsnTfa7"
   },
   "outputs": [],
   "source": [
    "# See this: https://github.com/liuzhuang13/slimming\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "# import tensorflow.python.keras as keras\n",
    "# import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.activations import linear, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl-areg_TtJd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "import tensorflow as tf\n",
    "\n",
    "class NSL(Layer):\n",
    "    # Neuron Selection Layer\n",
    "    def __init__(self, name=None, units=32, rate_l1=1e-3, rate_l2=0.0, initializer='ones', power=1.0, **kwargs):\n",
    "        super(NSL, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.rate_l1 = rate_l1\n",
    "        self.rate_l2 = rate_l2\n",
    "        self.initializer=initializer\n",
    "        self.power = power\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1]),\n",
    "                                initializer=self.initializer,\n",
    "                                trainable=True, \n",
    "                                regularizer=tf.keras.regularizers.l1_l2(l1=self.rate_l1, l2=self.rate_l2))\n",
    "        self.units = input_shape[-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(NSL, self).get_config()\n",
    "        config.update({\"units\": self.units, 'rate_l1': self.rate_l1, 'rate_l2': self.rate_l2, \n",
    "                       'initializer': self.initializer, 'power': self.power})\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.multiply(inputs, tf.math.pow(self.w, self.power))\n",
    "\n",
    "\n",
    "class CompressFlatten(Layer):\n",
    "    # To be used only for the flatten layer.\n",
    "    \n",
    "    def __init__(self, nsl_layer, threshold_to_remove=1e-2, **kwargs):\n",
    "        super(CompressFlatten, self).__init__(**kwargs)\n",
    "                \n",
    "        self.w_nsl = nsl_layer.get_weights()[0]\n",
    "        self.indices_to_keep = (np.where(abs(self.w_nsl)>=threshold_to_remove)[0])\n",
    "        \n",
    "    def build(self, input_shape):        \n",
    "        self.w = tf.Variable(self.w_nsl[self.indices_to_keep])  \n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CompressFlatten, self).get_config()            \n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        kept = tf.gather(inputs, self.indices_to_keep, axis=1)        \n",
    "        mul = tf.math.multiply(kept, self.w)\n",
    "        return mul\n",
    "\n",
    "def nsl_effect(model, threshold = 1e-2):\n",
    "    # Cacluclates how many neurons can be removed from the model bsed on the NSL layers and the threshold given\n",
    "    for l in range(len(model.layers)-1):\n",
    "        layer = model.layers[l]\n",
    "        if isinstance(layer, NSL):\n",
    "            print('Layer name: ', layer.name)\n",
    "            ws = layer.get_weights()[0]\n",
    "            print('Potential removable neurons: ', sum(np.abs(ws)<threshold))\n",
    "            print('Number of neurons: ', ws.shape[0])\n",
    "\n",
    "def zero_neurons(model, threshold=1e-2, loss=\"categorical_crossentropy\"):  \n",
    "    # Zeros out the weights it can zero based on NSL and threshold, given a model.\n",
    "    \n",
    "    model_cloned = keras.models.clone_model(model)\n",
    "    model_cloned.set_weights(model.get_weights())\n",
    "    \n",
    "    for i in range(len(model_cloned.layers)):\n",
    "        layer = model_cloned.layers[i]\n",
    "        \n",
    "        if isinstance(layer, NSL):\n",
    "            # print(layer)\n",
    "            ws = layer.get_weights()\n",
    "            sparsified_weights = []\n",
    "\n",
    "            for w in ws:\n",
    "                bool_mask = (abs(w) > threshold).astype(int)\n",
    "                sparsified_weights.append(w*bool_mask)\n",
    "\n",
    "            layer.set_weights(sparsified_weights)\n",
    "            \n",
    "    model_cloned.compile(loss=loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model_cloned\n",
    "            \n",
    "def evaluate_model(model, x_test, x_train, y_test, y_train):\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"Test loss:\", score[0])\n",
    "    print(\"Test accuracy:\", score[1])\n",
    "    score = model.evaluate(x_train, y_train, verbose=0)\n",
    "    print(\"Train loss:\", score[0])\n",
    "    print(\"Train accuracy:\", score[1])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uNfwP5vTsnM",
    "outputId": "e4bce57d-4bcc-4103-92fb-64242d9c8f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yqiybqXbTvtn",
    "outputId": "594b98f6-053a-4992-b0d0-df37c2b31b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "nsl_8 (NSL)                  (None, 26, 26, 32)        32        \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "nsl_9 (NSL)                  (None, 1600)              1600      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "nsl_10 (NSL)                 (None, 128)               128       \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "nsl_11 (NSL)                 (None, 128)               128       \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,434\n",
      "Trainable params: 243,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "422/422 [==============================] - 3s 5ms/step - loss: 2.3173 - accuracy: 0.8338 - val_loss: 1.4548 - val_accuracy: 0.9788\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 1.3230 - accuracy: 0.9769 - val_loss: 0.9173 - val_accuracy: 0.9805\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 0.7776 - accuracy: 0.9833 - val_loss: 0.4737 - val_accuracy: 0.9867\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.4270 - accuracy: 0.9858 - val_loss: 0.3419 - val_accuracy: 0.9858\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.3174 - accuracy: 0.9869 - val_loss: 0.2773 - val_accuracy: 0.9878\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.2717 - accuracy: 0.9873 - val_loss: 0.2541 - val_accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.2401 - accuracy: 0.9887 - val_loss: 0.2275 - val_accuracy: 0.9882\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.2167 - accuracy: 0.9895 - val_loss: 0.2104 - val_accuracy: 0.9877\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.1969 - accuracy: 0.9910 - val_loss: 0.1933 - val_accuracy: 0.9888\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 2s 5ms/step - loss: 0.1796 - accuracy: 0.9911 - val_loss: 0.1784 - val_accuracy: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdb90185630>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assumes:\n",
    "# - no NSL before flatten, \n",
    "# - At least one Dense after Flatten, \n",
    "# - only Conv2D, Flatten, and Dense are supported to be before NSL\n",
    "\n",
    "# In the NSL layers, initialization with 1 would make all neurons available for firing and starts trimming afterwards.\n",
    "# Initializatiob with 0 would start with minimmum number of zeros and then makes more firing to get better accuracy.\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "ac_reg = 0.0\n",
    "nu_reg = 1e-3\n",
    "\n",
    "input_shape=(28, 28, 1)\n",
    "\n",
    "inputs = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "x = keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"linear\")(inputs)\n",
    "x = NSL(rate_l1=nu_reg, rate_l2=0.0, initializer='ones')(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"linear\")(inputs)\n",
    "x = NSL(rate_l1=nu_reg, rate_l2=0.0, initializer='ones')(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"linear\")(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = NSL(rate_l1=nu_reg, rate_l2=0.0, initializer='ones')(x)\n",
    "\n",
    "x = keras.layers.Dense(128, activation='linear')(x)\n",
    "x = NSL(rate_l1=nu_reg, rate_l2=0.0, initializer='ones')(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.Dense(128,activation='relu')(x)\n",
    "x = NSL(rate_l1=nu_reg, rate_l2=0.0, initializer='ones')(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "# x = NeuronRegul(rate=1e-4)(x)\n",
    "# x = keras.layers.Dense(1024,activation='relu')(x)\n",
    "# x = NeuronRegul(rate=1e-4)(x)\n",
    "# x = NeuronRegul(rate=1e-4)(x)\n",
    "x = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=[inputs],outputs=[x])   \n",
    "\n",
    "model.summary()\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9ErH6OCUDfn",
    "outputId": "f0581991-c402-4d76-d1f5-f117e8d4b1da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance before trimming\n",
      "Test loss: 0.17277905344963074\n",
      "Test accuracy: 0.9890999794006348\n",
      "Train loss: 0.16724275052547455\n",
      "Train accuracy: 0.9922333359718323\n",
      "******\n",
      "Estimated effect of trimming on the number of neurons\n",
      "Layer name:  nsl_8\n",
      "Potential removable neurons:  2\n",
      "Number of neurons:  32\n",
      "Layer name:  nsl_9\n",
      "Potential removable neurons:  1376\n",
      "Number of neurons:  1600\n",
      "Layer name:  nsl_10\n",
      "Potential removable neurons:  37\n",
      "Number of neurons:  128\n",
      "Layer name:  nsl_11\n",
      "Potential removable neurons:  39\n",
      "Number of neurons:  128\n",
      "******\n",
      "Estimated accuracy after trimming\n",
      "Test loss: 0.17172950506210327\n",
      "Test accuracy: 0.9891999959945679\n",
      "Train loss: 0.16612540185451508\n",
      "Train accuracy: 0.9921666383743286\n"
     ]
    }
   ],
   "source": [
    "trimming_threshold = 1e-2\n",
    "\n",
    "print('Model performance before trimming')\n",
    "evaluate_model(model, x_test, x_train, y_test, y_train)\n",
    "\n",
    "print('******')\n",
    "print('Estimated effect of trimming on the number of neurons')\n",
    "nsl_effect(model, threshold = trimming_threshold)\n",
    "\n",
    "model_cloned = zero_neurons(model, threshold=trimming_threshold)\n",
    "\n",
    "print('******')\n",
    "print('Estimated accuracy after trimming')\n",
    "evaluate_model(model_cloned, x_test, x_train, y_test, y_train)\n",
    "\n",
    "# print('Number of parameters in the original model: ', model.count_params())\n",
    "# print('Number of parameters in the zeroed model: ', model_cloned.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "g8P-iuU1PvRL",
    "outputId": "78107b22-74cc-424c-a533-0fbc7f5f5709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_52\n",
      "conv2d_47\n",
      "nsl_18\n",
      "re_lu_9\n",
      "max_pooling2d_6\n",
      "conv2d_48\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-67970ff2443d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# newWeightList.append(biases)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mnew_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1871\u001b[0m           raise ValueError(\n\u001b[1;32m   1872\u001b[0m               \u001b[0;34m'Layer weight shape %s not compatible with provided weight '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m               'shape %s' % (ref_shape, weight.shape))\n\u001b[0m\u001b[1;32m   1874\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m         \u001b[0mweight_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer weight shape (3, 3, 27, 64) not compatible with provided weight shape (3, 3, 32, 64)"
     ]
    }
   ],
   "source": [
    "# NOT WORKING: THIS IS THE ACTUAL PHYSICAL REMOVAL OF NEURONS\n",
    "# Actual trimming and reduction of size\n",
    "# Assumes no NSL before flatten, Dense after Flatten, and only Conv2D and Dense are supported to be with NSL\n",
    "def build_layer(current_layer, current_model, units=None):\n",
    "    the_layer = None\n",
    "    weights = None\n",
    "    biases = None\n",
    "    print(current_layer)\n",
    "    \n",
    "    if isinstance(current_layer, layers.Conv2D) and (units is not None):\n",
    "        current_model = keras.layers.Conv2D(units.shape[0], kernel_size=current_layer.kernel_size,\n",
    "                                        activation=current_layer.activation)(current_model)\n",
    "        weights = current_layer.get_weights()[0][:,:,:,units]        \n",
    "        biases = current_layer.get_weights()[1][units]\n",
    "        print(weights.shape)\n",
    "\n",
    "    else:\n",
    "        if isinstance(current_layer, layers.Dense) and (units is not None):\n",
    "            current_model = keras.layers.Dense(units.shape[0], activation=current_layer.activation)(current_model)\n",
    "            weights = current_layer.get_weights()[0][units]\n",
    "#             print(weights.shape)\n",
    "            biases = current_layer.get_weights()[1][units]\n",
    "#             print(biases.shape)\n",
    "            \n",
    "        else:\n",
    "            current_model = current_layer.__class__.from_config(current_layer.get_config())(current_model)\n",
    "            weights = current_layer.get_weights()\n",
    "            if len(weights) > 0:\n",
    "                biases = current_layer.get_weights()[1]\n",
    "\n",
    "    return current_model, weights, biases\n",
    "\n",
    "\n",
    "inputs = keras.layers.Input(shape=input_shape)\n",
    "current_model = inputs\n",
    "# x = keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(inputs)\n",
    "built_net = False\n",
    "threshold = trimming_threshold\n",
    "all_ws = model.get_weights()\n",
    "newWeightList = []\n",
    "\n",
    "for l in range(len(model.layers) - 1):   \n",
    "    current_layer = model.layers[l]\n",
    "    next_layer = model.layers[l+1]\n",
    "    print(current_layer.name)\n",
    "\n",
    "    if isinstance(current_layer, NSL): # Remove NSL layers\n",
    "        continue        \n",
    "    \n",
    "    if isinstance(current_layer, layers.InputLayer): # Just keep this as it is\n",
    "        continue\n",
    "    \n",
    "    if isinstance(current_layer, layers.Flatten) and isinstance(next_layer, NSL): # Use CompressFlatten class to compress flatten layers\n",
    "        current_model = current_layer.__class__.from_config(current_layer.get_config())(current_model) # Copy flatten layer\n",
    "        # weights = current_layer.get_weights()\n",
    "        continue\n",
    "\n",
    "    if isinstance(current_layer, layers.Dense) and isinstance(next_layer, NSL): \n",
    "        # Neurons in the current layer would be trimmed according to the weights in the NSL layer next to it\n",
    "        ws = next_layer.get_weights()[0]\n",
    "        indices_to_keep = np.where(abs(ws)>=trimming_threshold)[0]\n",
    "\n",
    "        new_layer = keras.layers.Dense(indices_to_keep.shape[0], activation=current_layer.activation)\n",
    "        current_model = new_layer(current_model)\n",
    "\n",
    "        weights = current_layer.get_weights()[0][:, indices_to_keep]\n",
    "        biases = current_layer.get_weights()[1][indices_to_keep]\n",
    "        print(weights)\n",
    "        new_layer.set_weights([weights, biases])        \n",
    "\n",
    "        # print(weights.shape)\n",
    "        # newWeightList.append(weights)\n",
    "        # Append the new weight list with our sparsified bias weights\n",
    "        # newWeightList.append(biases)\n",
    "\n",
    "        continue\n",
    "\n",
    "    if isinstance(current_layer, layers.Conv2D) and isinstance(next_layer, NSL): \n",
    "        ws = next_layer.get_weights()[0]\n",
    "        indices_to_keep = np.where(abs(ws)>=trimming_threshold)[0]\n",
    "\n",
    "        new_layer = keras.layers.Conv2D(indices_to_keep.shape[0], kernel_size=current_layer.kernel_size,\n",
    "                                        activation=current_layer.activation)\n",
    "        current_model = new_layer(current_model)\n",
    "        weights = current_layer.get_weights()[0][:,:,:,indices_to_keep]        \n",
    "        biases = current_layer.get_weights()[1][indices_to_keep]\n",
    "\n",
    "        new_layer.set_weights([weights, biases])  \n",
    "\n",
    "        # print(weights.shape)\n",
    "        # newWeightList.append(weights)\n",
    "        # # Append the new weight list with our sparsified bias weights\n",
    "        # newWeightList.append(biases)\n",
    "\n",
    "        continue\n",
    "    \n",
    "    new_layer = current_layer.__class__.from_config(current_layer.get_config())\n",
    "    current_model = new_layer(current_model)\n",
    "\n",
    "    weights = current_layer.get_weights()  \n",
    "    biases = []\n",
    "    if len(weights) > 0:\n",
    "        # newWeightList.append(weights[0])\n",
    "        biases = weights[1]\n",
    "        # newWeightList.append(biases)\n",
    "        new_layer.set_weights([weights[0], biases])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "current_layer = model.layers[-1]\n",
    "current_model, ws, bs = build_layer(current_layer, current_model)\n",
    "newWeightList.append(ws)\n",
    "        \n",
    "# Append the new weight list with our sparsified bias weights\n",
    "newWeightList.append(bs)\n",
    "    \n",
    "model2 = Model(inputs=[inputs],outputs=[current_model])   \n",
    "# model2.set_weights(newWeightList)\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBLvOSunL0G6",
    "outputId": "a8268f4b-c5c6-49c3-cf76-a0769a3defbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.02895454, -0.02463328,  0.00964963, ..., -0.01220251,\n",
       "         -0.00771907,  0.00775115],\n",
       "        [ 0.0214486 ,  0.01004943,  0.06666191, ...,  0.04407688,\n",
       "         -0.01508951, -0.05085446],\n",
       "        [-0.07542598,  0.01191037,  0.01121551, ...,  0.00974657,\n",
       "          0.00251003, -0.00928916],\n",
       "        ...,\n",
       "        [ 0.00920593, -0.06717283, -0.00932135, ..., -0.04561407,\n",
       "          0.09944273,  0.02751794],\n",
       "        [-0.02082664, -0.09487423, -0.09315011, ..., -0.0398021 ,\n",
       "         -0.00121584,  0.00771366],\n",
       "        [-0.08467006,  0.09929226, -0.10167944, ..., -0.12786463,\n",
       "         -0.05055297,  0.04410986]], dtype=float32),\n",
       " array([ 0.005426  ,  0.02099917,  0.05814148,  0.01725162, -0.02143651,\n",
       "        -0.00157435, -0.01374769,  0.04859484,  0.02366389,  0.05425266,\n",
       "        -0.00879377, -0.01789035,  0.04629609,  0.04041431,  0.01899443,\n",
       "         0.04086782,  0.02657518,  0.01546485, -0.0280921 ,  0.06465711,\n",
       "        -0.02996951,  0.08198204, -0.0097654 , -0.00114272,  0.0433756 ,\n",
       "         0.0476503 ,  0.01919394,  0.05892349, -0.02927107,  0.03105676,\n",
       "         0.00052445,  0.01517131, -0.0013184 ,  0.00146265,  0.05158474,\n",
       "         0.00513646,  0.04715839, -0.00256595,  0.00217077, -0.0074035 ,\n",
       "         0.04174229,  0.06531363,  0.01987978,  0.00018591,  0.02039207,\n",
       "         0.06487513, -0.00789385, -0.01841593,  0.02608554,  0.03976482,\n",
       "        -0.00455175,  0.07393536,  0.05106887,  0.02156482,  0.01472142,\n",
       "         0.13083692,  0.0255474 ,  0.01724871,  0.01189607,  0.02827875,\n",
       "        -0.00093915,  0.0350383 ,  0.03591885, -0.02164923,  0.01053873,\n",
       "        -0.03143775,  0.02922552,  0.06368919,  0.06862999, -0.01464183,\n",
       "         0.02622973,  0.00235954,  0.01780945,  0.08012062, -0.00253868,\n",
       "         0.02928271,  0.00821682,  0.05274528,  0.0047613 ,  0.00300194,\n",
       "         0.04969673, -0.00855251, -0.02370622,  0.00214233,  0.02301686,\n",
       "         0.02463736,  0.03559822,  0.06755877,  0.00639675,  0.04054391,\n",
       "         0.01198085,  0.05198332,  0.00798809,  0.0409817 , -0.01609318,\n",
       "        -0.00313131,  0.01039541,  0.03757786,  0.01401619], dtype=float32)]"
      ]
     },
     "execution_count": 145,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.layers[8].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2BCghrXMpEj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "DHcBJ5bd6ju8",
    "outputId": "7c5972e5-1fac-4d71-9e90-2bb0ecadd224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 32)\n",
      "(32,)\n",
      "(3, 3, 32, 64)\n",
      "(64,)\n",
      "(99, 128)\n",
      "(99,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-b68266f7798b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model2.layers[8].set_weights(model2.layers[8].get_weights())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewWeightList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# model2.layers[8].set_weights(model2.layers[8].get_weights())\n",
    "for l in newWeightList:\n",
    "    print(l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYubx4mPLR-f",
    "outputId": "af14543f-c396-46f0-dbe0-ea11db4edcd4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.005426  ,  0.02099917,  0.05814148,  0.01725162, -0.02143651,\n",
       "       -0.00157435, -0.01374769,  0.04859484,  0.02366389,  0.05425266,\n",
       "       -0.00879377, -0.01789035,  0.04629609,  0.04041431,  0.01899443,\n",
       "        0.04086782,  0.02657518,  0.01546485, -0.0280921 ,  0.06465711,\n",
       "       -0.02996951,  0.08198204, -0.0097654 , -0.00114272,  0.0433756 ,\n",
       "        0.0476503 ,  0.01919394,  0.05892349, -0.02927107,  0.03105676,\n",
       "        0.00052445,  0.01517131, -0.0013184 ,  0.00146265,  0.05158474,\n",
       "        0.00513646,  0.04715839, -0.00256595,  0.00217077, -0.0074035 ,\n",
       "        0.04174229,  0.06531363,  0.01987978,  0.00018591,  0.02039207,\n",
       "        0.06487513, -0.00789385, -0.01841593,  0.02608554,  0.03976482,\n",
       "       -0.00455175,  0.07393536,  0.05106887,  0.02156482,  0.01472142,\n",
       "        0.13083692,  0.0255474 ,  0.01724871,  0.01189607,  0.02827875,\n",
       "       -0.00093915,  0.0350383 ,  0.03591885, -0.02164923,  0.01053873,\n",
       "       -0.03143775,  0.02922552,  0.06368919,  0.06862999, -0.01464183,\n",
       "        0.02622973,  0.00235954,  0.01780945,  0.08012062, -0.00253868,\n",
       "        0.02928271,  0.00821682,  0.05274528,  0.0047613 ,  0.00300194,\n",
       "        0.04969673, -0.00855251, -0.02370622,  0.00214233,  0.02301686,\n",
       "        0.02463736,  0.03559822,  0.06755877,  0.00639675,  0.04054391,\n",
       "        0.01198085,  0.05198332,  0.00798809,  0.0409817 , -0.01609318,\n",
       "       -0.00313131,  0.01039541,  0.03757786,  0.01401619], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newWeightList[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jC7nJHOG_US",
    "outputId": "4e5ec69f-51c9-4a1a-eee3-4ad2edb5e901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_10\n",
      "conv2d_8 (3, 3, 1, 32) (32,)\n",
      "nsl_6 (32,)\n",
      "re_lu_3\n",
      "max_pooling2d_2\n",
      "conv2d_9 (3, 3, 32, 64) (64,)\n",
      "re_lu_4\n",
      "max_pooling2d_3\n",
      "flatten_1\n",
      "nsl_7 (1600,)\n",
      "dense_7 (1600, 128) (128,)\n",
      "nsl_8 (128,)\n",
      "re_lu_5\n",
      "dense_8 (128, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "for l in model.layers:\n",
    "    ws = l.get_weights()\n",
    "    if(len(ws) == 0):        \n",
    "        print(l.name)\n",
    "    if(len(ws) == 2):        \n",
    "        print(l.name, ws[0].shape, ws[1].shape)\n",
    "    if(len(ws) == 1):        \n",
    "        print(l.name, ws[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzLGMdsx5Qqq",
    "outputId": "0f8224cd-b63d-41c6-bc90-9c8e4b3b8aa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.03468296,  0.02145528,  0.05375824, ..., -0.02545723,\n",
       "         -0.01086837, -0.0438729 ],\n",
       "        [ 0.05731425,  0.03655965,  0.02118937, ...,  0.04780157,\n",
       "          0.04346022,  0.01605261],\n",
       "        [-0.03042637, -0.02921392,  0.02016058, ..., -0.03738361,\n",
       "          0.04409149,  0.01345651],\n",
       "        ...,\n",
       "        [-0.03584927, -0.01987287, -0.04095186, ..., -0.02748552,\n",
       "          0.05216978, -0.00818924],\n",
       "        [ 0.03606356, -0.03985491,  0.00380449, ...,  0.00318469,\n",
       "          0.05794683, -0.04804903],\n",
       "        [ 0.03771136,  0.03024045,  0.00701199, ..., -0.03929397,\n",
       "         -0.02187249,  0.04765125]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = model2.layers[8]\n",
    "l.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dR3w94DgCpiv",
    "outputId": "a84e24c1-b29c-411b-be9c-d34ccf6bd3ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f331ffd8be0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f331ffd8b38>,\n",
       " <tensorflow.python.keras.layers.advanced_activations.ReLU at 0x7f3320308e10>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f331ffb9748>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f331ffd83c8>,\n",
       " <tensorflow.python.keras.layers.advanced_activations.ReLU at 0x7f3320125e80>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f331ffa88d0>,\n",
       " <tensorflow.python.keras.layers.core.Flatten at 0x7f331ffdf780>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f331ff8f4a8>,\n",
       " <tensorflow.python.keras.layers.advanced_activations.ReLU at 0x7f331ffa8cc0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f331ff8f898>]"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SY_u4iljugn-",
    "outputId": "8d23996d-761d-4afe-ee15-7d1cca99b533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 117.43372344970703\n",
      "Test accuracy: 0.03739999979734421\n",
      "Train loss: 116.51602172851562\n",
      "Train accuracy: 0.03689999878406525\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "evaluate_model(model2, x_test, x_train, y_test, y_train)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "regularize_neurons.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
